{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c407183-9192-4b38-8812-284f861f24ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a51c2e-39fc-4972-86b4-421701d92576",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset():\n",
    "    face_classifier = cv2.CascadeClassifier(\"C:\\\\Users\\\\hp\\\\OneDrive\\\\Desktop\\\\face recognition\\\\haarcascade_frontalface_default.xml\")\n",
    "    def face_cropped(img):\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        faces = face_classifier.detectMultiScale(gray,1.3,5)\n",
    "        #scaling factor = 1.3\n",
    "        #Minimum neighbour = 5\n",
    "\n",
    "        if faces is ():\n",
    "            return None\n",
    "        for(x,y,w,h) in faces:\n",
    "            cropped_face=img[y:y+h,x:x+w]\n",
    "        return cropped_face\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    id=1\n",
    "    img_id=0\n",
    "\n",
    "    while True:\n",
    "        ret,frame = cap.read()\n",
    "        if face_cropped(frame) is not None:\n",
    "            img_id+=1\n",
    "            face = cv2.resize(face_cropped(frame),(200,200))\n",
    "            face = cv2.cvtColor(face, cv2.COLOR_BGR2GRAY)\n",
    "            #file_name_path = \"data/user.\"+str(id)+\".\"+str(img_id)+\".jpg\"\n",
    "            file_name_path = \"Images for visualization/\"+str(img_id)+'.jpg'\n",
    "            cv2.imwrite(file_name_path,face)\n",
    "            cv2.putText(face,str(img_id),(50,50),cv2.FONT_HERSHEY_COMPLEX,1, (0,255,0))\n",
    "            #(50,50) IS THE ORIGIN POINT FROM WHERE TEXT IS TO BE WRITTEN\n",
    "            # FONT SCALE=1\n",
    "            #thickness=2\n",
    "\n",
    "            cv2.imshow(\"Cropped face\",face)\n",
    "            if cv2.waitKey(1)==13 or int(img_id)==20:\n",
    "                break\n",
    "                cap.release()\n",
    "                cv2.destroyALLWindows()\n",
    "                print(\"collecting sample is completed........\")\n",
    "generate_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e74a7d0-6df3-422d-b214-50c62709b82a",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'cv2' has no attribute 'face'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 26\u001b[0m\n\u001b[0;32m     24\u001b[0m     clf\u001b[38;5;241m.\u001b[39mtrain(faces,ids)\n\u001b[0;32m     25\u001b[0m     clf\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclassifier.xml\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 26\u001b[0m train_classifier(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[11], line 23\u001b[0m, in \u001b[0;36mtrain_classifier\u001b[1;34m(data_dir)\u001b[0m\n\u001b[0;32m     20\u001b[0m ids \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(ids)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Train and save classifier\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m clf \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mface\u001b[38;5;241m.\u001b[39mLBPHFaceRecognizer_create()\n\u001b[0;32m     24\u001b[0m clf\u001b[38;5;241m.\u001b[39mtrain(faces,ids)\n\u001b[0;32m     25\u001b[0m clf\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclassifier.xml\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'cv2' has no attribute 'face'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "from PIL import Image #pip install pillow\n",
    "import numpy as np    # pip install numpy\n",
    " \n",
    "def train_classifier(data_dir):\n",
    "    path = [os.path.join(data_dir, f) for f in os.listdir(data_dir)]\n",
    "     \n",
    "    faces = []\n",
    "    ids = []\n",
    "     \n",
    "    for image in path:\n",
    "        img = Image.open(image).convert('L')\n",
    "        imageNp = np.array(img, 'uint8')\n",
    "        id = int(os.path.split(image)[1].split(\".\")[1])\n",
    "         \n",
    "        faces.append(imageNp)\n",
    "        ids.append(id)\n",
    "         \n",
    "    ids = np.array(ids)\n",
    "     \n",
    "    # Train and save classifier\n",
    "    clf = cv2.face.LBPHFaceRecognizer_create()\n",
    "    clf.train(faces,ids)\n",
    "    clf.write(\"classifier.xml\")\n",
    "train_classifier(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1e6c4f-4d1c-4293-b4e9-0a5b9a10cd73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    " \n",
    "def draw_boundary(img, classifier, scaleFactor, minNeighbors, color, text, clf):\n",
    "    gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    features = classifier.detectMultiScale(gray_img, scaleFactor, minNeighbors)\n",
    "     \n",
    "    for (x,y,w,h) in features:\n",
    "        cv2.rectangle(img, (x,y), (x+w,y+h), color, 2 )\n",
    "         \n",
    "        id, pred = clf.predict(gray_img[y:y+h,x:x+w])\n",
    "        confidence = int(100*(1-pred/300))\n",
    "         \n",
    "        if confidence>70:\n",
    "            if id==1:\n",
    "                cv2.putText(img, \"Diya\", (x,y-5), cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 1, cv2.LINE_AA)\n",
    "            if id==2:\n",
    "                cv2.putText(img, \"Manvi\", (x,y-5), cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 1, cv2.LINE_AA)\n",
    "        else:\n",
    "            cv2.putText(img, \"UNKNOWN\", (x,y-5), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0,0,255), 1, cv2.LINE_AA)\n",
    "     \n",
    "    return img\n",
    " \n",
    "# loading classifier\n",
    "faceCascade = cv2.CascadeClassifier(\"haarcascade_frontalface_default.xml\")\n",
    " \n",
    "clf = cv2.face.LBPHFaceRecognizer_create()\n",
    "clf.read(\"classifier.xml\")\n",
    " \n",
    "video_capture = cv2.VideoCapture(0)\n",
    " \n",
    "while True:\n",
    "    ret, img = video_capture.read()\n",
    "    img = draw_boundary(img, faceCascade, 1.3, 6, (255,255,255), \"Face\", clf)\n",
    "    cv2.imshow(\"face Detection\", img)\n",
    "     \n",
    "    if cv2.waitKey(1)==13:\n",
    "        break\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12e71c8-7b85-477a-9175-09c5d6701c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3248d006-2c3e-433c-9ed8-495a31212152",
   "metadata": {},
   "outputs": [],
   "source": [
    "mydb=mysql.connector.connect(\n",
    "    host=\"localhost\",\n",
    "    port=3306,\n",
    "    user=\"root\",\n",
    "    passwd=\"\" \n",
    ")\n",
    "print(mydb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074e14a7-683b-46a0-be4d-b542ae36c0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mycursor=mydb.cursor()\n",
    "mycursor.execute(\"CREATE DATABASE Authorized_user\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b218ec-1af8-4807-804d-da97e476cf9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mycursor.execute(\"SHOW DATABASES\")\n",
    "for x in mycursor:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10ccfb5-6eec-470f-9543-11db14c27fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "\n",
    "# Establish connection without specifying the database\n",
    "mydb = mysql.connector.connect(\n",
    "    host=\"localhost\",\n",
    "    user=\"root\",\n",
    "    passwd=\"\" \n",
    ")\n",
    "\n",
    "# Create a cursor object\n",
    "mycursor = mydb.cursor()\n",
    "\n",
    "# Create the database if it doesn't exist\n",
    "mycursor.execute(\"CREATE DATABASE IF NOT EXISTS Authorised_user\")\n",
    "print(\"Database created successfully or already exists.\")\n",
    "\n",
    "# Reconnect to the database\n",
    "mydb = mysql.connector.connect(\n",
    "    host=\"localhost\",\n",
    "    user=\"root\",\n",
    "    passwd=\"\", \n",
    "    database=\"Authorised_user\"\n",
    ")\n",
    "\n",
    "# Create a cursor for the new connection\n",
    "mycursor = mydb.cursor()\n",
    "\n",
    "# Execute SQL query to create a table\n",
    "mycursor.execute(\"CREATE TABLE IF NOT EXISTS my_table (id INT PRIMARY KEY, Name VARCHAR(50), Age INT, Address VARCHAR(50))\")\n",
    "print(\"Table created successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ee38cc5-cebc-4099-aa2f-be3b1628f48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import *\n",
    "import cv2\n",
    "import os\n",
    "from PIL import Image, ImageTk\n",
    "import numpy as np\n",
    "import mysql.connector\n",
    "from tkinter import messagebox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6582c3-0dbf-4ad8-923b-39b53bfbdd34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "No motion detected for 5 seconds.\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n",
      "Motion detected!\n"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import PhotoImage, messagebox\n",
    "from PIL import Image, ImageTk\n",
    "import cv2\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "import mysql.connector\n",
    "\n",
    "# Initialize the main window\n",
    "window = tk.Tk()\n",
    "window.title(\"SafeGuard AI\")\n",
    "window.resizable(0, 0)\n",
    "\n",
    "# Load and retain images properly to prevent garbage collection\n",
    "load1 = Image.open(\"C:/Users/hp/OneDrive/Desktop/face recognition/SAFEGUARD AI (1).jpg\")\n",
    "photo1 = ImageTk.PhotoImage(load1)\n",
    "window.photo1 = photo1  # Retain image reference to prevent garbage collection\n",
    "\n",
    "header = tk.Button(window, image=photo1)\n",
    "header.place(x=5, y=0)\n",
    "\n",
    "canvas1 = tk.Canvas(window, width=500, height=250, bg='ivory')\n",
    "canvas1.place(x=5, y=120)\n",
    "\n",
    "l1 = tk.Label(canvas1, text=\"Name\", font=(\"Georgia\", 15))\n",
    "l1.place(x=5, y=5)\n",
    "t1 = tk.Entry(canvas1, width=50, bd=5)\n",
    "t1.place(x=150, y=10)\n",
    "\n",
    "l2 = tk.Label(canvas1, text=\"Age\", font=(\"Georgia\", 15))\n",
    "l2.place(x=5, y=50)\n",
    "t2 = tk.Entry(canvas1, width=50, bd=5)\n",
    "t2.place(x=150, y=55)\n",
    "\n",
    "l3 = tk.Label(canvas1, text=\"Address\", font=(\"Georgia\", 15))\n",
    "l3.place(x=5, y=100)\n",
    "t3 = tk.Entry(canvas1, width=50, bd=5)\n",
    "t3.place(x=150, y=105)\n",
    "\n",
    "# Load second image and retain it\n",
    "load2 = Image.open(\"C:/Users/hp/OneDrive/Desktop/face recognition/canvas1.jpg\")\n",
    "photo2 = ImageTk.PhotoImage(load2)\n",
    "window.photo2 = photo2  # Prevent garbage collection\n",
    "\n",
    "canvas2 = tk.Canvas(window, width=500, height=250)\n",
    "canvas2.place(x=5, y=400)\n",
    "canvas2.create_image(250, 125, image=photo2)\n",
    "\n",
    "# Load third image and retain it\n",
    "load3 = Image.open(\"C:/Users/hp/OneDrive/Desktop/face recognition/canvas2.jpg\")\n",
    "photo3 = ImageTk.PhotoImage(load3)\n",
    "window.photo3 = photo3  # Prevent garbage collection\n",
    "\n",
    "canvas3 = tk.Canvas(window, width=280, height=530)\n",
    "canvas3.place(x=515, y=120)\n",
    "canvas3.create_image(140, 265, image=photo3)\n",
    "\n",
    "\n",
    "# Face Detection Functionality\n",
    "def detect_face_and_motion():\n",
    "    def draw_boundary(img, classifier, scaleFactor, minNeighbors, color, text, clf):\n",
    "        gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        features = classifier.detectMultiScale(gray_img, scaleFactor, minNeighbors)\n",
    "\n",
    "        for (x, y, w, h) in features:\n",
    "            cv2.rectangle(img, (x, y), (x + w, y + h), color, 2)\n",
    "\n",
    "            id, pred = clf.predict(gray_img[y:y + h, x:x + w])\n",
    "            confidence = int(100 * (1 - pred / 300))\n",
    "\n",
    "            # Fetch name from database\n",
    "            mydb = mysql.connector.connect(\n",
    "                host=\"localhost\",\n",
    "                user=\"root\",\n",
    "                passwd=\"\",\n",
    "                database=\"authorized_user\"\n",
    "            )\n",
    "            mycursor = mydb.cursor()\n",
    "            mycursor.execute(\"SELECT name FROM my_table WHERE id = %s\", (id,))\n",
    "            result = mycursor.fetchone()\n",
    "            name = result[0] if result else \"Unknown\"\n",
    "\n",
    "            if confidence > 74:\n",
    "                cv2.putText(img, name, (x, y - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 1, cv2.LINE_AA)\n",
    "            else:\n",
    "                cv2.putText(img, \"UNKNOWN\", (x, y - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "\n",
    "        return img\n",
    "\n",
    "    faceCascade = cv2.CascadeClassifier(\"haarcascade_frontalface_default.xml\")\n",
    "    clf = cv2.face_LBPHFaceRecognizer.create()\n",
    "\n",
    "    clf.read(\"classifier.xml\")\n",
    "\n",
    "    video_capture = cv2.VideoCapture(0)\n",
    "\n",
    "    time_without_motion = 0\n",
    "\n",
    "    ret, prev_frame = video_capture.read()\n",
    "    if not ret:\n",
    "        print(\"Failed to capture video frame.\")\n",
    "        video_capture.release()\n",
    "        return\n",
    "\n",
    "    prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)\n",
    "    prev_gray = cv2.GaussianBlur(prev_gray, (21, 21), 0)\n",
    "\n",
    "    while True:\n",
    "        ret, img = video_capture.read()\n",
    "        if not ret:\n",
    "            print(\"Failed to capture video frame.\")\n",
    "            break\n",
    "\n",
    "        # Face Detection\n",
    "        img = draw_boundary(img, faceCascade, 1.3, 6, (255, 255, 255), \"Face\", clf)\n",
    "\n",
    "        # Motion Detection\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        gray = cv2.GaussianBlur(gray, (21, 21), 0)\n",
    "\n",
    "        frame_delta = cv2.absdiff(prev_gray, gray)\n",
    "        thresh = cv2.threshold(frame_delta, 25, 255, cv2.THRESH_BINARY)[1]\n",
    "        thresh = cv2.dilate(thresh, None, iterations=2)\n",
    "\n",
    "        contours, _ = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        motion_detected = any(cv2.contourArea(contour) > 5000 for contour in contours)\n",
    "\n",
    "        if motion_detected:\n",
    "            time_without_motion = 0\n",
    "            print(\"Motion detected!\")\n",
    "        else:\n",
    "            time_without_motion += 1\n",
    "\n",
    "        if time_without_motion >= 150:  # 5 seconds at 30 FPS\n",
    "            print(\"No motion detected for 5 seconds.\")\n",
    "            time.sleep(5)\n",
    "            time_without_motion = 0\n",
    "\n",
    "        # Display the combined result\n",
    "        cv2.imshow(\"Face and Motion Detection\", img)\n",
    "        prev_gray = gray.copy()\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):  # Press 'q' to exit\n",
    "            break\n",
    "\n",
    "    video_capture.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "# Training Functionality\n",
    "def train_classifier():\n",
    "    data_dir = \"C:/Users/hp/OneDrive/Desktop/face recognition/data\"\n",
    "    path = [os.path.join(data_dir, f) for f in os.listdir(data_dir)]\n",
    "\n",
    "    faces = []\n",
    "    ids = []\n",
    "\n",
    "    for image in path:\n",
    "        img = Image.open(image).convert('L')\n",
    "        imageNp = np.array(img, 'uint8')\n",
    "        id = int(os.path.split(image)[1].split(\".\")[1])  # Extract ID from filename\n",
    "        faces.append(imageNp)\n",
    "        ids.append(id)\n",
    "\n",
    "    ids = np.array(ids)\n",
    "\n",
    "    clf = cv2.face.LBPHFaceRecognizer_create()\n",
    "    clf.train(faces, ids)\n",
    "    clf.write(\"classifier.xml\")\n",
    "    messagebox.showinfo('Result', 'Training dataset completed!!!')\n",
    "\n",
    "\n",
    "# Motion Detection Functionality\n",
    "\n",
    "\n",
    "\n",
    "# Dataset Generation Functionality\n",
    "def generate_dataset():\n",
    "    if t1.get() == \"\" or t2.get() == \"\" or t3.get() == \"\":\n",
    "        messagebox.showinfo('Result', 'Please provide complete details of the user')\n",
    "    else:\n",
    "        try:\n",
    "            mydb = mysql.connector.connect(\n",
    "                host=\"localhost\",\n",
    "                user=\"root\",\n",
    "                passwd=\"\"\n",
    "            )\n",
    "            mycursor = mydb.cursor()\n",
    "            mycursor.execute(\"CREATE DATABASE IF NOT EXISTS authorized_user\")\n",
    "            mydb.database = \"authorized_user\"\n",
    "\n",
    "            mycursor.execute(\"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS my_table (\n",
    "                    id INT PRIMARY KEY,\n",
    "                    Name VARCHAR(50),\n",
    "                    Age INT,\n",
    "                    Address VARCHAR(50)\n",
    "                )\n",
    "            \"\"\")\n",
    "\n",
    "            mycursor.execute(\"SELECT MAX(id) FROM my_table\")\n",
    "            max_id = mycursor.fetchone()[0]\n",
    "            id = (max_id + 1) if max_id else 1\n",
    "\n",
    "            sql = \"INSERT INTO my_table(id, Name, Age, Address) VALUES (%s, %s, %s, %s)\"\n",
    "            val = (id, t1.get(), t2.get(), t3.get())\n",
    "            mycursor.execute(sql, val)\n",
    "            mydb.commit()\n",
    "\n",
    "            face_classifier = cv2.CascadeClassifier(\"haarcascade_frontalface_default.xml\")\n",
    "\n",
    "            def face_cropped(img):\n",
    "                gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "                faces = face_classifier.detectMultiScale(gray, 1.3, 5)\n",
    "                if len(faces) == 0:\n",
    "                    return None\n",
    "                for (x, y, w, h) in faces:\n",
    "                    return img[y:y + h, x:x + w]  # Return the first detected face\n",
    "\n",
    "\n",
    "            cap = cv2.VideoCapture(0)\n",
    "            img_id = 0\n",
    "\n",
    "            while True:\n",
    "                ret, frame = cap.read()\n",
    "                cropped_face = face_cropped(frame)\n",
    "                if cropped_face is not None:\n",
    "                    img_id += 1\n",
    "                    face = cv2.resize(cropped_face, (200, 200))\n",
    "                    face = cv2.cvtColor(face, cv2.COLOR_BGR2GRAY)\n",
    "                    file_name_path = f\"data/user.{id}.{img_id}.jpg\"\n",
    "                    cv2.imwrite(file_name_path, face)\n",
    "                    cv2.imshow(\"Cropped Face\", face)\n",
    "\n",
    "                if cv2.waitKey(1) == 13 or img_id == 200:\n",
    "                    break\n",
    "\n",
    "            cap.release()\n",
    "            cv2.destroyAllWindows()\n",
    "            messagebox.showinfo('Result', 'Dataset generation completed!')\n",
    "\n",
    "        except mysql.connector.Error as e:\n",
    "            print(f\"Database Error: {e}\")\n",
    "\n",
    "\n",
    "# Buttons for functionalities\n",
    "b1 = tk.Button(canvas1, text=\"Training\", font=(\"Georgia\", 20), bg='#7E1891', fg='white', command=train_classifier)\n",
    "b1.place(x=10, y=180)\n",
    "\n",
    "b2 = tk.Button(canvas1, text=\"Generate Dataset\", font=(\"Georgia\", 20), bg='#7E1891', fg='white', command=generate_dataset)\n",
    "b2.place(x=175, y=180)\n",
    "\n",
    "b3 = tk.Button(canvas2, text=\"Detect Face & Motion\", font=(\"Georgia\", 20), bg=\"#7E1891\", fg=\"white\", command=detect_face_and_motion)\n",
    "b3.place(x=5, y=100)\n",
    "\n",
    "\n",
    "# Finalizing the window layout\n",
    "window.geometry(\"800x680\")\n",
    "window.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6ea23b-d88a-4796-a8fe-a67facb4750b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
